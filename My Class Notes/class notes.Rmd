---
title: "First Class"
author: "Noah Love"
date: "1/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
n <- 100
p <- 5
X <- matrix(runif(p * n),
            nrow=n)
beta <- rep(0, p)
beta[sample(p, 1)] <- 1
noise <- rnorm(n)

#Matrix multiplication in R
#X is a matrix of 100 by 5
#Beta is a matrix of 5
Y <- X %*% beta + noise

#ols means ordinary least squares

```

If we don't know

```{r}
ols <- lm(Y~X)
summary(ols)
```

Shows the truth:
```{r}
beta
```

Lets look at the y and subtract the the coeffisients to find the residual:
```{r}
resid <-  Y - cbind(1, X) %*% ols$coefficients
#this is the manual version of:
resid <-  ols$residuals
```

Now letsdo the residuals against the true beta:

```{r}
resid_from_truth <- Y - X %*% beta
```


```{r}
plot(density(resid_from_truth))
lines(density(resid), col = "red")
```

Let's see the difference between the two:

```{r}
mean(abs(resid))
mean(abs(resid_from_truth))
```
We want the smaller value. The simulated data will always have a smaller. The fitted coefficients will always be better than the "true" coefficients. Why is this bad?

Normally we want to use regresion to find the natural coefficients or natural facts about the world. These are bassed on some "truth". If you can collect noisey data but our algo prefers the fitted data rather than the truth we have a problem. We want our error minimized at the "truth". Our regression here doesn't like the true answer and it prefers something else. It actually prefers the training data. 

This is because we use the same data to train and evaluate. Our data is just optimized for this one thing .



Now lets generate another set of new Y using the same beta and x but with new noise values:
```{r}
#Testing data generated from the same population but not used for training the model
new_noise <- rnorm(n)
new_Y <- X %*% beta + new_noise
new_resid <- new_Y - cbind(1, X) %*% ols$coefficients
new_resid_from_truth <-  new <- Y - X %*% beta
mean(abs(resid))
mean(abs(resid_from_truth))

```


##Non linear data

```{r}
n <- 100
X <- runif(100, max = 2 * pi / 4 * 3)
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)

summary(lm(Y ~ X))
plot(X, Y)
abline(lm(Y ~ X))

ols <- lm(Y ~ X)
plot(X, ols$residuals) #should make you say we need to add a quadratic term
abline(h = 0)
#par(mfrom=c(1,2))
#to plot side by side


#Y - y hat squared
mean((Y - ols$fitted.values) ^ 2)

i <- 0
while()


```

**
Plot the MSE vs the number of polynomials used on the x axis. Do this for both the training vs the testing data. **

```{r}
## Our Code
i <- 1
MSEvec <- rep(0, 10)

while (i < 11) {
  loopOls <- lm(Y ~ poly(X, i))
  loopMSE <- mean((Y - loopOls$fitted.values) ^ 2)
  MSEvec[i] <- loopMSE
  i <- i + 1
}

i <- 1
MSEvec2 <- rep(0, 10)

while (i < 11) {
  loopOls <- lm(new_Y ~ poly(X, i))
  loopMSE <- mean((new_Y - loopOls$fitted.values) ^ 2)
  MSEvec2[i] <- loopMSE
  i <- i + 1
}

plot(MSEvec)
plot(MSEvec2)
```
```{r}

#Wayne's code
n <- 100
degrees <- 1:50

X <- runif(n, max = 2 * pi / 4 * 3)
Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
new_Y <- 0.1+-3 * sin(X) + rnorm(n, sd = 0.5)
X_mat <- sapply(degrees, function(i)
  X ^ i)


MSEs <- rep(NA, length(degrees))
test_MSEs <- MSEs
for (i in seq_along(degrees)) {
  ols <- lm(Y ~ X_mat[, 1:i])
  MSEs[i] <- mean(ols$residuals ^ 2)
  new_errors <- new_Y - ols$fitted.values
  test_MSEs[i] <- mean(new_errors ^ 2)
}
plot(degrees, MSEs, type = "b",
     ylim = c(0, max(test_MSEs)))
lines(degrees, test_MSEs, type = "b", col = "red")
legend("topright",
       legend = c("Test", "Train"),
       fill = c("red", "black"))


```

```{r}
summary(ols)
```
This should be really concerning. It would mean strong colinearity. R is doing us a favor and automatically dropping some redundant features.


