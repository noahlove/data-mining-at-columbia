---
title: "Regression Review"
author: "Noah Love"
date: "2/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
Create data under the so called regression model. Regression is an algo, but it is also a model for creating data. ODS is a least sqaure algoristhm. 

```{r}
#God knows the true parameters, but unknown to us humans
# n <- 100
# p <- 5
# params <- runif(p, -10, 10)     #
# 
# features <- matrix(rnorm(n* (p-1)),nrow = n,ncol = p-1) #one is column of one, or the intercept
# 
# X <- cbind(1, features)
# 
# #now combine the parameters with the X and add some noise
# 
# noise <- rnorm(n)
# Y <- X  %*% params + noise
# #Same as
# Y2 <- 0
# for(i in seq_len(p)){
#   params[i] * X[,i]
# }
# Y2 <- Y2 + noise
# 
# dim(X)
# 
# plot(Y,Y2)

```


```{r}
#Wayne's code
n <- 100
p <- 5
# God knows this!
params <- runif(p, -10, 10)

features <- matrix(rnorm(n * p),
                   nrow=n, ncol=p)
X <- features
X1 <- matrix(rnorm(n * p),
              nrow=n, ncol=p)

noise <- rnorm(n)
noise1 <- rnorm(n)
Y1 <- X %*% params + noise
Y2 <- 0
for(i in seq_len(p)){
  Y2 <- Y2 + params[i] * X1[, i]
}
Y2 <- Y2 + noise1
plot(X[,1], Y1)
points(X1[, 1], Y2, col="red")


```


```{r}
df <- as.data.frame(cbind(Y,X))

dim(df)

head(df,2)

names(df) <- c("Y", paste0("X", 1:p))

ols <- lm(Y ~ ., df)
summary(ols)

class(ols)
predict(ols)

#if you pass in a lm or glm, predict will use predict.lm or predict.glm anyways. It is smart. 

#There is an argument in predict that uses "new data". You need to pass in what the new data is. It should be tempting for us to just pass in X1. 
#This shouldn't work! But why?
#predict(ols, newdata = as.data.frame(X1))

#convert to df

#now there is an error that we don't know what X2 is. The data frame you are passing it needs to have the same names that you are training on. 

names(df)

df1  <- as.data.frame(X1)
names(df1) <- names(df)[-1]

test_preds <- predict(
  ols, 
  newdata = df1)

#The data you are passing, you need the data to look identical to the data you trained the model with. The names of the data frames must agree. 
#classic workflow
plot(test_preds , Y2)

test_errors <- Y2 - test_preds
test_rmse <- sqrt(mean(test_errors ^ 2))
test_rmse


sd(noise)
sd(noise1)
#You cannot reduce beyond this. 

```
R will automatically throw out extremely high colinearity instances. In the real world this would be rare. This is unique to R. 


```{r}
names(ols)

#probably the most important
ols$coefficients

plot(ols$coefficients, c(0,params))
abline(a = 0, b=1)

```

```{r}
train_features <- cbind(1,as.matrix(df[,-1])) #take out y column
#fitted_vals <- train_features %*% 
  
fitted_vals <- train_features %*% ols$coefficients
sum(abs(fitted_vals - ols$fitted.values))

res <- df$Y - ols$fitted.values
sum(abs(res - ols$residuals))
plot(ols$residuals)
abline(h = 0)

```

```{r}
#you can also put plot onto the regression function itself
plot(ols)
#residuals vs fitted values. This is what we saw earlier but much fancier

#QQ Plot

#scale location not that important

#leverage to look for outliers
```
#### Traps
```{r}
df_missing <- df
df_missing[20, "Y"]  <- NA #purposeffully lose the value
ols <- lm(Y ~., df_missing)
length(ols$residuals)
#lm drops missing value before matrix multiplication. So the residuals will change. 


```

### Interactions + subtracting variables


#### Another trap

```{r}
ols <- lm(Y ~ X1 + X2 + X2*X3, df)
ols <- lm(Y ~ X1 + X2 + X2:X3, df)

summary(ols)
test_preds <- predict(ols, df1)
head(df1,2)
#when you start manipulating the data inbetween then you get the problems

#If you do feature engineering for test, then ADD SOMETHING
```


```{r}
ols <- lm(Y ~ . - X4, df)
summary(ols)

#get rid of intercept
ols <- lm(Y  ~ . -1, df)
summary(ols)
```

INSERT STUFF ABOUT INTERACTION TERMS: COLON THING


## Last trap, ording of the data
```{r}
x <- runif(n, -2 ,2)
y <- x^2 * 3

ols <-  lm(y~x)
summary(ols)

predict(ols, data.frame(x = runif(n,-2,2)))

predict(ols, data.frame(x = runif(n, -2,2)))

plot(x,y)

plot(ols$residuals)
#we should expect the residuals to be quadratic as well
# we need to order the data correctly

plot(ols$fitted.values,
     ols$residuals)

#remember the reisdualds are ordered the same as the data. If the data was random, then the residuals will be random. 

plot(ols)

#naming comment
#If we decided to 
dim(X)
length(Y)


ols <- lm(Y ~ X[,1:2])
summary(ols)
#the naming is a mess!

wrong_stuff <- predict(ols, data.frame("X[, 1:2]" = 1:3,
                        "X[, 1:2]2" = 1:3))
                        

mysterious_vals <- predict(ols)                       
sum(abs(mysterious_vals - ols$fitted.values))
```

### Missing

If there is no overlap in the data. Then there is no overlapping data. You can't run regression in that case. 

you should probably have a reason to say you need a certain amount of overlap. The best way to get there is to find the needed level of confidence and then back in the answer. 

### GLM
```{r}
inv_logit <- function(x) {
  return(exp(x) / (1 + exp(x)))
}
y <- rbinom(n, 1, prob = inv_logit(X %*% params))
plot(X[, 1], y)
plot(X[, 2], y)

df <- as.data.frame(cbind(y, X))
names(df) <- c("Y", paste0("X", 1:5))
log_reg <- glm(Y ~ ., df,
               family = binomial(logit))

summary(log_reg)
predict(log_reg)
myst_vals <- predict(log_reg, type = "response")

X_test <- matrix(rnorm(n * p), nrow = n)
X_test_df <- as.data.frame(X_test)
names(X_test_df) <- names(df)[-1]

test_preds <-
  predict(log_reg, type = "response", newdata = X_test_df)
head(test_preds)
params


library(randomForest)

rf(Y ~ ., df)

```

